{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hotel_Reviews.csv']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "from textblob import TextBlob\n",
    "from sklearn import datasets, linear_model\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "\n",
    "# Load the diabetes dataset\n",
    "import os\n",
    "print(os.listdir(\"./data\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsRawData = pd.read_csv(\"./data/Hotel_Reviews.csv\", usecols=['Positive_Review', 'Negative_Review', 'Reviewer_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative_Review    515738\n",
       "Positive_Review    515738\n",
       "Reviewer_Score     515738\n",
       "dtype: int64"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make sure there's no empty attribute\n",
    "reviewsRawData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#since there's no empty attribute, we can seperate each column to convert reviews to numberic values\n",
    "reviewsRawDataPos = reviewsRawData.Positive_Review[:, np.newaxis]\n",
    "#print(reviewsRawDataPos)\n",
    "reviewsRawDataNeg = reviewsRawData.Negative_Review[:, np.newaxis]\n",
    "#reviewsRawDataNeg = reviewsRawData.Negative_Review[0:10, np.newaxis]\n",
    "\n",
    "#print(reviewsRawDataPos)\n",
    "#print()\n",
    "#print(reviewsRawDataNeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.006132296721140544 minutes ---\n",
      "\n",
      "[-148.33333333333334, 66.0, -23.0, -96.33333333333334, -57.66666666666667, 1.0, -10.0, 11.0, -21.0, 12.0, 69.0, -31.0, -24.0, 34.0, -2.0, 50.333333333333336, -19.0, 0.0, 18.0, 6.0, -36.0, -20.0, -76.2, 22.0, 77.33333333333333, -8.0, 74.33333333333333, -16.0, -1.0, 32.333333333333336, 17.0, -10.0, -44.0, 11.0, 9.0, 25.0, -16.0, 15.0, 83.0, -53.0, -4.0, -45.0, -72.66666666666667, -5.0, -4.0, -6.0, -183.0, -2.0, 33.0, -29.0, -5.0, -46.0, 43.0, 48.0, -2.0, 29.0, -26.0, 20.0, 1.0, 39.666666666666664, -98.0, -4.0, 24.000000000000007, -15.0, -2.0, -14.0, -13.0, 46.0, -17.0, 0.0, -14.0, 2.0, -12.0, -2.0000000000000018, -60.25, 24.0, 1.0, 1.0, 35.0, 19.0, -13.0, 19.0, -36.0, -29.333333333333336, -21.0, 24.0, -29.0, -58.0, -5.0, -8.0, -97.0, -24.0, -15.0, -96.58333333333334, -24.0, 53.0, 18.0, 11.0, -53.0, -27.0, -32.0, -12.0, -10.666666666666666, -5.0, 10.0, 9.0, -18.0, -9.0, 14.0, 6.0, 6.0, 44.0, 73.0, 11.0, 46.0, -22.0, -10.0, 53.0, -29.666666666666664, -10.0, -24.0, -13.0, -7.0, 17.0, -70.0, 0.0, -17.0, -3.0, 10.0, 4.0, 3.0, 9.0, -32.0, -4.0, -103.0, 22.0, -4.0, -9.0, -1.0, 10.0, 37.0, 13.0, 2.0, -5.0, -4.0, -10.0, -108.0, -18.0, 3.0, 9.0, -5.0, -21.0, -1.0, 0.0, 24.0, -24.0, -27.0, -17.750000000000007, -9.0, -32.0, 9.0, -16.0, -17.0, 17.0, -20.0, 42.0, -5.0, -14.0, 13.0, -21.0, -14.0, -22.0, -25.0, -11.0, 46.0, -21.0, 4.0, 7.0, 41.0, -4.0, -13.666666666666666, -36.0, -41.333333333333336, 15.0, 3.0, -41.0, 9.0, -60.0, -21.0, -46.0, -34.0, -34.0, 22.0, -9.0, -18.0, -26.0, -32.0, 2.0, 30.0, -65.0, 32.0, -12.0, -17.0, -31.0, 31.0, -46.85714285714286, -26.0, 29.5, -8.0, -68.0, 25.0, 13.0, -21.0, -2.0, 3.0, 18.0, 24.0, 9.0, 4.0, -15.0, 40.0, -30.0, 40.0, 25.0, 19.0, 12.0, 73.66666666666667, -38.0, 31.0, 73.0, 11.0, 19.0, -59.0, -23.0, 42.0, 8.0, 24.0, 21.0, -32.0, -22.0, -31.0, 25.0, 20.0, 0.0, -21.0, 29.0, 4.0, -2.0, -5.0, 1.0, 3.0, 21.0, 3.0, -1.0, 19.0, 22.0, -1.0, 0.0, -4.0, -28.0, 0.0, -2.0, -19.0, 4.0, 0.0, 1.0, -2.0, -5.0, -1.0, 4.0, -4.0, 3.0, 4.0, 15.0, 6.0, 12.0, 6.0, 5.0, -1.0, 13.0, -18.0, 7.0, 11.0, 3.0, 2.0, -1.0, -1.0, -3.0, -1.0, 1.0, 2.0, 4.0, -14.0, 9.0, 1.0, -1.0, -6.0, 1.0, -3.0, -30.0, -3.0, -9.0, 2.0, -5.0, 4.0, 7.0, -4.0, -13.0, 9.0, -6.0, 9.0, 9.0, 4.0, 6.0, 5.0, 3.0, -1.0, -4.0, 15.0, 0.0, 5.0, 8.0, -4.0, 8.0, 0, 0.0, 1.0, 7.0, -3.0, 0.0, -5.0, -4.0, 2.0, -1.0, -7.0, 1.0, -6.0, 5.0, 3.0, 10.0, -9.0, -2.0, 4.0, 6.0, 13.0, 3.0, -7.0, 0.0, 13.0, -2.0, -6.0, -13.0, 3.0, 5.0, 8.0, 11.0, 12.0, -6.0, 3.0, 4.0, 11.0, 1.0, 1.0, 1.0, -6.0, -5.0, 8.0, 2.0, -1.0, -2.0, 3.0, 21.0, 13.0, 3.0, 8.0, 3.0, -8.0, 2.0, 18.0, -8.0, 13.0, 5.0, 8.0, 3.0, -8.0, 10.0, -4.0, 8.0, -1.0, -8.0, 7.0, -3.0, 14.0, -7.0, 6.0, 6.0, 9.0, 6.0, -1.0, -7.0, -4.0, -2.0, 4.0, 2.0, 7.0, -2.0, -23.0, 6.0, 54.0, 4.0, 43.0, 35.0, -3.8000000000000007, -14.0, 18.0, -59.0, -7.0, 6.0, -22.0, 36.0, -54.0, -25.999999999999996, -34.0, 34.0, 16.0, 1.0, -34.0, 26.0, 16.0, 24.0, 10.0, -16.0, 12.0, 19.0, 41.0, 24.25, -14.0, 14.000000000000004, 16.0, 10.0, 76.0, 12.0, -4.0, -13.0, -10.0, 4.0, -54.0, -22.666666666666664, 32.0, 0.0, 15.0, 2.0, -21.0, 33.0, 32.0, 47.0, 8.0, 25.0, 6.0, 18.0, -28.0, 15.0, -9.0, -15.0, 73.0, 4.0, -2.0, -20.0, -13.0, 7.0, 4.0, 4.0, -55.0, -15.0, -13.0, -10.0, -4.0, -45.0, -7.0, -21.0, 8.333333333333336, -11.0, 12.0, -9.0, 10.0, 8.0, 114.0, -51.0, 25.0, 7.0, 12.0, -2.0, -21.0, 39.0, 38.333333333333336, 10.0, -16.0, -20.0, 11.0, -32.0, -15.0, 15.0, 57.0, 5.0, 17.0, 27.0, 42.666666666666664, 19.0, 19.0, -30.0, 12.0, 41.0, 9.0, 2.0, -3.0, -29.25, 27.0, -7.0, 33.0, 35.0, -10.0, -20.0, 18.0, 3.0, -23.0, -50.0, -8.666666666666666, 1.0, 21.0, -3.0, 40.0, 3.0, 21.0, -13.0, -2.0, 17.0, -62.0, -12.0, -41.0, 5.0, 25.666666666666664, -7.0, 14.0, 16.0, -139.78571428571428, -19.0, 9.0, -7.0, 16.0, 28.0, -2.0, 52.99999999999999, 34.0, 2.0, 48.0, -16.0, -2.0, -2.0, 31.66666666666667, 1.0, 12.0, 18.0, 58.66666666666667, 1.0, 37.0, -4.0, 29.0, -6.0, -13.0, 23.0, 2.0, 30.0, 7.0, 28.0, 3.0, 16.0, 40.0, 23.0, 15.0, -12.0, 7.0, -18.0, 9.0, 17.0, 14.0, 7.0, 24.0, -11.0, -17.0, 55.0, 15.0, 15.0, 17.0, -11.0, 2.0, -9.0, 5.0, -1.0, -6.0, 10.0, -23.0, -15.0, 12.0, 36.0, -5.0, 8.0, 18.0, 37.0, -38.0, 5.0, -4.0, 5.0, 39.2, 12.0, -38.0, 15.0, 24.0, -8.0, -1.0, -32.0, -27.0, -8.0, -33.0, -8.0, 21.0, 19.0, 22.0, -7.0, -30.0, -18.0, 10.0, -8.0, 25.0, 19.0, 13.0, 20.0, -30.0, 23.0, -15.0, -7.0, 5.0, 29.0, -13.0, -13.0, 34.0, 32.0, 41.66666666666667, 5.0, 12.0, -38.0, 23.0, 14.0, -35.5, 23.0, 65.0, -69.5, -30.0, -3.0, -19.0, -16.0, -29.0, 22.0, 0.0, 44.0, -3.0, -7.000000000000002, 18.0, 5.0, -87.66666666666667, -6.0, -11.0, 0.0, 37.0, 2.0, 14.0, -6.0, -17.0, 1.0, 15.0, -22.0, 25.0, 39.0, 7.0, -59.00000000000001, -78.0, 8.0, 8.0, -1.0, -1.0, 17.0, 1.0, 5.0, -8.0, 8.0, 3.0, 9.0, 22.0, -4.0, 7.0, -2.0, -7.0, -10.0, 7.0, 3.0, -6.0, -6.0, -1.0, 17.0, 1.0, -7.0, -12.0, 0.0, -2.0, 0.0, -7.0, -3.0, 4.0, -3.0, 7.0, 0.0, 3.0, -3.0, -7.0, -3.0, -7.0, -8.0, -3.0, 17.0, 1.0, -15.0, -1.0, 8.0, 3.0, 4.0, 4.0, 19.0, 9.0, -2.0, -1.0, 2.0, 4.0, -1.0, -6.0, 5.0, -1.0, 8.0, 12.0, 14.0, 3.0, -11.0, -4.0, 5.0, 6.0, -1.0, -11.0, -1.0, -3.0, -1.0, 7.0, 1.0, 5.0, 9.0, 23.0, -6.0, 4.0, 4.0, 3.0, 8.0, 7.0, 0.0, 8.0, -8.0, 12.0, -2.0, 3.0, 0.0, 0.0, 8.0, 7.0, 8.0, 5.0, 3.0, 3.0, 5.0, 0.0, -11.0, 2.0, 0.0, 1.0, -1.0, -13.0, 7.0, -3.0, 11.0, 9.0, 4.0, -1.0, 3.0, -1.0, 17.0, -5.0, 4.0, 5.0, 1.0, 2.0, 5.0, -6.0, 13.0, 3.0, 10.0, -6.0, 0.0, -1.0, 3.0, -2.0, 7.0, 14.0, -2.0, -1.0, 7.0, 3.0, -4.0, 4.0, 5.0, -1.0, 9.0, -5.0, 18.0, 1.0, -5.0, 0, 2.0, 10.0, 10.0, -25.666666666666664, 7.0, 11.0, 6.0, 14.0, 7.0, -1.0, -2.0, 1.0, -16.0, -1.0, 2.0, 0.0, -2.0, 6.0, 1.0, 15.0, -5.0, -2.0, 10.0, 1.0, -7.0, 2.0, -7.0, -1.0, -1.0, -5.0, -1.0, 5.0, 0.0, 15.0, -13.0, -6.0, 18.0, 2.0, 6.0, -1.0, -1.0, 9.0, -6.0, -3.0, -5.0, -2.0, 0.0, 21.0, 10.0, 13.0, 9.0, -5.0, -2.0, 13.0, -6.0, 17.0, 1.0, 8.0, -5.0, -3.0, -2.0, -1.0, 0.0, 1.0, -11.0, -11.0, 3.0, 7.0, 3.0, 12.0, -7.0, -1.0, 8.0, -1.0, -4.0, -4.0, 3.0, 7.0, 12.0, 4.0, -5.0, -11.0, 3.0, 0.0, 1.0, 4.0, 7.0, -5.0, -12.0, -3.0, 4.0, 10.0, 7.0, 13.0, 8.0, 0.0, 14.0, 0.0, 9.0, 14.0, 3.0, -7.0, 16.0, 0.0, 0.0, -1.0, -5.0, 10.0, -2.0, 27.0, 13.0, 1.0, 17.0, 10.0, -1.0, 14.0, -13.0, 10.0, 0.0, 5.0, 10.0, 3.0, 13.0, -13.0, -7.0, 1.0, 2.0, -1.0, 1.0, -9.0, 14.0, -1.0, 1.0, 4.0, -1.0, 20.0, 7.0, 13.0, 17.0, 1.0, -1.0, 22.0, -19.0, -20.0, 14.0, -15.0, 21.0, -12.0, 44.66666666666667, 31.0, 9.0, -2.0, 7.0, 14.0, 30.0, -52.0, 30.0, 32.0, -7.0, 4.0, 40.0, 12.0, 11.0, 25.0, 32.0, -12.0, -24.5, -1.0, 22.0]\n"
     ]
    }
   ],
   "source": [
    "#make empty (0) array of size of reviews\n",
    "reviewsNumData = [0] * len(reviewsRawData)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#convert positive review to numeric value\n",
    "# using the sum of both positive and negative keywords' weights \n",
    "\n",
    "for i in range(0, len(reviewsRawData)):\n",
    "    # get important keywords with weight for each positive review\n",
    "    posKeywords = rake.apply(reviewsRawDataPos[i][0])\n",
    "    # each keyword's weight gets added up\n",
    "    for eachTuple in posKeywords:\n",
    "        reviewsNumData[i]= reviewsNumData[i] + eachTuple[1]\n",
    "    \n",
    "    # get important keywords with weight for each negative review\n",
    "    negKeywords = rake.apply(reviewsRawDataNeg[i][0])\n",
    "    # each keyword's weight gets subtracted\n",
    "    for eachTuple in negKeywords:\n",
    "        reviewsNumData[i]= reviewsNumData[i] - eachTuple[1]\n",
    "\n",
    "    #print(reviewsNumData[i])\n",
    "\n",
    "# con: due to size of the data, conversion takes time (~5mins)\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))\n",
    "print()\n",
    "print(reviewsNumData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part was made with referencing Linear regression - OLS vs GD\n",
    "# Split the data into training/testing sets\n",
    "reviews_X_train = reviewsNumData[:-100000] # training set = all but last 100000 instances\n",
    "reviews_X_test = reviewsNumData[-100000:]  #test set = last 100000 instances\n",
    "\n",
    "#extract reviewScore data\n",
    "reviewerScore = reviewsRawData.Reviewer_Score[:, np.newaxis]\n",
    "\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "reviews_Y_train = reviewerScore[:-100000]  \n",
    "reviews_Y_test = reviewerScore[-100000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-148.33333333   66.          -23.          -96.33333333  -57.66666667\n    1.          -10.           11.          -21.           12.\n   69.          -31.          -24.           34.           -2.\n   50.33333333  -19.            0.           18.            6.\n  -36.          -20.          -76.2          22.           77.33333333\n   -8.           74.33333333  -16.           -1.           32.33333333\n   17.          -10.          -44.           11.            9.\n   25.          -16.           15.           83.          -53.\n   -4.          -45.          -72.66666667   -5.           -4.\n   -6.         -183.           -2.           33.          -29.\n   -5.          -46.           43.           48.           -2.\n   29.          -26.           20.            1.           39.66666667\n  -98.           -4.           24.          -15.           -2.\n  -14.          -13.           46.          -17.            0.\n  -14.            2.          -12.           -2.          -60.25\n   24.            1.            1.           35.           19.\n  -13.           19.          -36.          -29.33333333  -21.\n   24.          -29.          -58.           -5.           -8.\n  -97.          -24.          -15.          -96.58333333  -24.\n   53.           18.           11.          -53.          -27.\n  -32.          -12.          -10.66666667   -5.           10.\n    9.          -18.           -9.           14.            6.\n    6.           44.           73.           11.           46.\n  -22.          -10.           53.          -29.66666667  -10.\n  -24.          -13.           -7.           17.          -70.\n    0.          -17.           -3.           10.            4.\n    3.            9.          -32.           -4.         -103.\n   22.           -4.           -9.           -1.           10.\n   37.           13.            2.           -5.           -4.\n  -10.         -108.          -18.            3.            9.\n   -5.          -21.           -1.            0.           24.\n  -24.          -27.          -17.75         -9.          -32.\n    9.          -16.          -17.           17.          -20.\n   42.           -5.          -14.           13.          -21.\n  -14.          -22.          -25.          -11.           46.\n  -21.            4.            7.           41.           -4.\n  -13.66666667  -36.          -41.33333333   15.            3.\n  -41.            9.          -60.          -21.          -46.\n  -34.          -34.           22.           -9.          -18.\n  -26.          -32.            2.           30.          -65.\n   32.          -12.          -17.          -31.           31.\n  -46.85714286  -26.           29.5          -8.          -68.\n   25.           13.          -21.           -2.            3.\n   18.           24.            9.            4.          -15.\n   40.          -30.           40.           25.           19.\n   12.           73.66666667  -38.           31.           73.\n   11.           19.          -59.          -23.           42.\n    8.           24.           21.          -32.          -22.\n  -31.           25.           20.            0.          -21.\n   29.            4.           -2.           -5.            1.\n    3.           21.            3.           -1.           19.\n   22.           -1.            0.           -4.          -28.\n    0.           -2.          -19.            4.            0.\n    1.           -2.           -5.           -1.            4.\n   -4.            3.            4.           15.            6.\n   12.            6.            5.           -1.           13.\n  -18.            7.           11.            3.            2.\n   -1.           -1.           -3.           -1.            1.\n    2.            4.          -14.            9.            1.\n   -1.           -6.            1.           -3.          -30.\n   -3.           -9.            2.           -5.            4.\n    7.           -4.          -13.            9.           -6.\n    9.            9.            4.            6.            5.\n    3.           -1.           -4.           15.            0.\n    5.            8.           -4.            8.            0.\n    0.            1.            7.           -3.            0.\n   -5.           -4.            2.           -1.           -7.\n    1.           -6.            5.            3.           10.\n   -9.           -2.            4.            6.           13.\n    3.           -7.            0.           13.           -2.\n   -6.          -13.            3.            5.            8.\n   11.           12.           -6.            3.            4.\n   11.            1.            1.            1.           -6.\n   -5.            8.            2.           -1.           -2.\n    3.           21.           13.            3.            8.\n    3.           -8.            2.           18.           -8.\n   13.            5.            8.            3.           -8.\n   10.           -4.            8.           -1.           -8.\n    7.           -3.           14.           -7.            6.\n    6.            9.            6.           -1.           -7.\n   -4.           -2.            4.            2.            7.\n   -2.          -23.            6.           54.            4.\n   43.           35.           -3.8         -14.           18.\n  -59.           -7.            6.          -22.           36.\n  -54.          -26.          -34.           34.           16.\n    1.          -34.           26.           16.           24.\n   10.          -16.           12.           19.           41.\n   24.25        -14.           14.           16.           10.\n   76.           12.           -4.          -13.          -10.\n    4.          -54.          -22.66666667   32.            0.\n   15.            2.          -21.           33.           32.\n   47.            8.           25.            6.           18.\n  -28.           15.           -9.          -15.           73.\n    4.           -2.          -20.          -13.            7.\n    4.            4.          -55.          -15.          -13.\n  -10.           -4.          -45.           -7.          -21.\n    8.33333333  -11.           12.           -9.           10.\n    8.          114.          -51.           25.            7.\n   12.           -2.          -21.           39.           38.33333333\n   10.          -16.          -20.           11.          -32.\n  -15.           15.           57.            5.           17.\n   27.           42.66666667   19.           19.          -30.\n   12.           41.            9.            2.           -3.\n  -29.25         27.           -7.           33.           35.\n  -10.          -20.           18.            3.          -23.\n  -50.           -8.66666667    1.           21.           -3.\n   40.            3.           21.          -13.           -2.\n   17.          -62.          -12.          -41.            5.\n   25.66666667   -7.           14.           16.         -139.78571429\n  -19.            9.           -7.           16.           28.\n   -2.           53.           34.            2.           48.\n  -16.           -2.           -2.           31.66666667    1.\n   12.           18.           58.66666667    1.           37.\n   -4.           29.           -6.          -13.           23.\n    2.           30.            7.           28.            3.\n   16.           40.           23.           15.          -12.\n    7.          -18.            9.           17.           14.\n    7.           24.          -11.          -17.           55.\n   15.           15.           17.          -11.            2.\n   -9.            5.           -1.           -6.           10.\n  -23.          -15.           12.           36.           -5.\n    8.           18.           37.          -38.            5.\n   -4.            5.           39.2          12.          -38.\n   15.           24.           -8.           -1.          -32.\n  -27.           -8.          -33.           -8.           21.\n   19.           22.           -7.          -30.          -18.\n   10.           -8.           25.           19.           13.\n   20.          -30.           23.          -15.           -7.\n    5.           29.          -13.          -13.           34.\n   32.           41.66666667    5.           12.          -38.\n   23.           14.          -35.5          23.           65.\n  -69.5         -30.           -3.          -19.          -16.\n  -29.           22.            0.           44.           -3.\n   -7.           18.            5.          -87.66666667   -6.\n  -11.            0.           37.            2.           14.\n   -6.          -17.            1.           15.          -22.\n   25.           39.            7.          -59.          -78.\n    8.            8.           -1.           -1.           17.\n    1.            5.           -8.            8.            3.\n    9.           22.           -4.            7.           -2.\n   -7.          -10.            7.            3.           -6.\n   -6.           -1.           17.            1.           -7.\n  -12.            0.           -2.            0.           -7.\n   -3.            4.           -3.            7.            0.\n    3.           -3.           -7.           -3.           -7.\n   -8.           -3.           17.            1.          -15.\n   -1.            8.            3.            4.            4.\n   19.            9.           -2.           -1.            2.\n    4.           -1.           -6.            5.           -1.\n    8.           12.           14.            3.          -11.\n   -4.            5.            6.           -1.          -11.\n   -1.           -3.           -1.            7.            1.\n    5.            9.           23.           -6.            4.\n    4.            3.            8.            7.            0.\n    8.           -8.           12.           -2.            3.\n    0.            0.            8.            7.            8.\n    5.            3.            3.            5.            0.\n  -11.            2.            0.            1.           -1.\n  -13.            7.           -3.           11.            9.\n    4.           -1.            3.           -1.           17.\n   -5.            4.            5.            1.            2.\n    5.           -6.           13.            3.           10.\n   -6.            0.           -1.            3.           -2.\n    7.           14.           -2.           -1.            7.\n    3.           -4.            4.            5.           -1.\n    9.           -5.           18.            1.           -5.\n    0.            2.           10.           10.          -25.66666667\n    7.           11.            6.           14.            7.\n   -1.           -2.            1.          -16.           -1.\n    2.            0.           -2.            6.            1.\n   15.           -5.           -2.           10.            1.\n   -7.            2.           -7.           -1.           -1.\n   -5.           -1.            5.            0.           15.\n  -13.           -6.           18.            2.            6.\n   -1.           -1.            9.           -6.           -3.\n   -5.           -2.            0.           21.           10.\n   13.            9.           -5.           -2.           13.\n   -6.           17.            1.            8.           -5.\n   -3.           -2.           -1.            0.            1.\n  -11.          -11.            3.            7.            3.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-325-45697dec0c47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Train the model using the training sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mregr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreviews_Y_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##this is where \"learning\" happens!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# The coefficients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m--> 458\u001b[1;33m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    745\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    748\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    545\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-148.33333333   66.          -23.          -96.33333333  -57.66666667\n    1.          -10.           11.          -21.           12.\n   69.          -31.          -24.           34.           -2.\n   50.33333333  -19.            0.           18.            6.\n  -36.          -20.          -76.2          22.           77.33333333\n   -8.           74.33333333  -16.           -1.           32.33333333\n   17.          -10.          -44.           11.            9.\n   25.          -16.           15.           83.          -53.\n   -4.          -45.          -72.66666667   -5.           -4.\n   -6.         -183.           -2.           33.          -29.\n   -5.          -46.           43.           48.           -2.\n   29.          -26.           20.            1.           39.66666667\n  -98.           -4.           24.          -15.           -2.\n  -14.          -13.           46.          -17.            0.\n  -14.            2.          -12.           -2.          -60.25\n   24.            1.            1.           35.           19.\n  -13.           19.          -36.          -29.33333333  -21.\n   24.          -29.          -58.           -5.           -8.\n  -97.          -24.          -15.          -96.58333333  -24.\n   53.           18.           11.          -53.          -27.\n  -32.          -12.          -10.66666667   -5.           10.\n    9.          -18.           -9.           14.            6.\n    6.           44.           73.           11.           46.\n  -22.          -10.           53.          -29.66666667  -10.\n  -24.          -13.           -7.           17.          -70.\n    0.          -17.           -3.           10.            4.\n    3.            9.          -32.           -4.         -103.\n   22.           -4.           -9.           -1.           10.\n   37.           13.            2.           -5.           -4.\n  -10.         -108.          -18.            3.            9.\n   -5.          -21.           -1.            0.           24.\n  -24.          -27.          -17.75         -9.          -32.\n    9.          -16.          -17.           17.          -20.\n   42.           -5.          -14.           13.          -21.\n  -14.          -22.          -25.          -11.           46.\n  -21.            4.            7.           41.           -4.\n  -13.66666667  -36.          -41.33333333   15.            3.\n  -41.            9.          -60.          -21.          -46.\n  -34.          -34.           22.           -9.          -18.\n  -26.          -32.            2.           30.          -65.\n   32.          -12.          -17.          -31.           31.\n  -46.85714286  -26.           29.5          -8.          -68.\n   25.           13.          -21.           -2.            3.\n   18.           24.            9.            4.          -15.\n   40.          -30.           40.           25.           19.\n   12.           73.66666667  -38.           31.           73.\n   11.           19.          -59.          -23.           42.\n    8.           24.           21.          -32.          -22.\n  -31.           25.           20.            0.          -21.\n   29.            4.           -2.           -5.            1.\n    3.           21.            3.           -1.           19.\n   22.           -1.            0.           -4.          -28.\n    0.           -2.          -19.            4.            0.\n    1.           -2.           -5.           -1.            4.\n   -4.            3.            4.           15.            6.\n   12.            6.            5.           -1.           13.\n  -18.            7.           11.            3.            2.\n   -1.           -1.           -3.           -1.            1.\n    2.            4.          -14.            9.            1.\n   -1.           -6.            1.           -3.          -30.\n   -3.           -9.            2.           -5.            4.\n    7.           -4.          -13.            9.           -6.\n    9.            9.            4.            6.            5.\n    3.           -1.           -4.           15.            0.\n    5.            8.           -4.            8.            0.\n    0.            1.            7.           -3.            0.\n   -5.           -4.            2.           -1.           -7.\n    1.           -6.            5.            3.           10.\n   -9.           -2.            4.            6.           13.\n    3.           -7.            0.           13.           -2.\n   -6.          -13.            3.            5.            8.\n   11.           12.           -6.            3.            4.\n   11.            1.            1.            1.           -6.\n   -5.            8.            2.           -1.           -2.\n    3.           21.           13.            3.            8.\n    3.           -8.            2.           18.           -8.\n   13.            5.            8.            3.           -8.\n   10.           -4.            8.           -1.           -8.\n    7.           -3.           14.           -7.            6.\n    6.            9.            6.           -1.           -7.\n   -4.           -2.            4.            2.            7.\n   -2.          -23.            6.           54.            4.\n   43.           35.           -3.8         -14.           18.\n  -59.           -7.            6.          -22.           36.\n  -54.          -26.          -34.           34.           16.\n    1.          -34.           26.           16.           24.\n   10.          -16.           12.           19.           41.\n   24.25        -14.           14.           16.           10.\n   76.           12.           -4.          -13.          -10.\n    4.          -54.          -22.66666667   32.            0.\n   15.            2.          -21.           33.           32.\n   47.            8.           25.            6.           18.\n  -28.           15.           -9.          -15.           73.\n    4.           -2.          -20.          -13.            7.\n    4.            4.          -55.          -15.          -13.\n  -10.           -4.          -45.           -7.          -21.\n    8.33333333  -11.           12.           -9.           10.\n    8.          114.          -51.           25.            7.\n   12.           -2.          -21.           39.           38.33333333\n   10.          -16.          -20.           11.          -32.\n  -15.           15.           57.            5.           17.\n   27.           42.66666667   19.           19.          -30.\n   12.           41.            9.            2.           -3.\n  -29.25         27.           -7.           33.           35.\n  -10.          -20.           18.            3.          -23.\n  -50.           -8.66666667    1.           21.           -3.\n   40.            3.           21.          -13.           -2.\n   17.          -62.          -12.          -41.            5.\n   25.66666667   -7.           14.           16.         -139.78571429\n  -19.            9.           -7.           16.           28.\n   -2.           53.           34.            2.           48.\n  -16.           -2.           -2.           31.66666667    1.\n   12.           18.           58.66666667    1.           37.\n   -4.           29.           -6.          -13.           23.\n    2.           30.            7.           28.            3.\n   16.           40.           23.           15.          -12.\n    7.          -18.            9.           17.           14.\n    7.           24.          -11.          -17.           55.\n   15.           15.           17.          -11.            2.\n   -9.            5.           -1.           -6.           10.\n  -23.          -15.           12.           36.           -5.\n    8.           18.           37.          -38.            5.\n   -4.            5.           39.2          12.          -38.\n   15.           24.           -8.           -1.          -32.\n  -27.           -8.          -33.           -8.           21.\n   19.           22.           -7.          -30.          -18.\n   10.           -8.           25.           19.           13.\n   20.          -30.           23.          -15.           -7.\n    5.           29.          -13.          -13.           34.\n   32.           41.66666667    5.           12.          -38.\n   23.           14.          -35.5          23.           65.\n  -69.5         -30.           -3.          -19.          -16.\n  -29.           22.            0.           44.           -3.\n   -7.           18.            5.          -87.66666667   -6.\n  -11.            0.           37.            2.           14.\n   -6.          -17.            1.           15.          -22.\n   25.           39.            7.          -59.          -78.\n    8.            8.           -1.           -1.           17.\n    1.            5.           -8.            8.            3.\n    9.           22.           -4.            7.           -2.\n   -7.          -10.            7.            3.           -6.\n   -6.           -1.           17.            1.           -7.\n  -12.            0.           -2.            0.           -7.\n   -3.            4.           -3.            7.            0.\n    3.           -3.           -7.           -3.           -7.\n   -8.           -3.           17.            1.          -15.\n   -1.            8.            3.            4.            4.\n   19.            9.           -2.           -1.            2.\n    4.           -1.           -6.            5.           -1.\n    8.           12.           14.            3.          -11.\n   -4.            5.            6.           -1.          -11.\n   -1.           -3.           -1.            7.            1.\n    5.            9.           23.           -6.            4.\n    4.            3.            8.            7.            0.\n    8.           -8.           12.           -2.            3.\n    0.            0.            8.            7.            8.\n    5.            3.            3.            5.            0.\n  -11.            2.            0.            1.           -1.\n  -13.            7.           -3.           11.            9.\n    4.           -1.            3.           -1.           17.\n   -5.            4.            5.            1.            2.\n    5.           -6.           13.            3.           10.\n   -6.            0.           -1.            3.           -2.\n    7.           14.           -2.           -1.            7.\n    3.           -4.            4.            5.           -1.\n    9.           -5.           18.            1.           -5.\n    0.            2.           10.           10.          -25.66666667\n    7.           11.            6.           14.            7.\n   -1.           -2.            1.          -16.           -1.\n    2.            0.           -2.            6.            1.\n   15.           -5.           -2.           10.            1.\n   -7.            2.           -7.           -1.           -1.\n   -5.           -1.            5.            0.           15.\n  -13.           -6.           18.            2.            6.\n   -1.           -1.            9.           -6.           -3.\n   -5.           -2.            0.           21.           10.\n   13.            9.           -5.           -2.           13.\n   -6.           17.            1.            8.           -5.\n   -3.           -2.           -1.            0.            1.\n  -11.          -11.            3.            7.            3.        ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# This part was made with referencing Linear regression - OLS vs GD\n",
    "# Create linear regression object\n",
    "\n",
    "regr = linear_model.LinearRegression() #run ordinary least squares\n",
    "\n",
    "#regr=linear_model.SGDRegressor(loss='squared_loss', alpha=0.01, max_iter=60) #Gradient Descent \n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(reviews_X_train, reviews_Y_train) ##this is where \"learning\" happens!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
